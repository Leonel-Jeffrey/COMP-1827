{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Convolutional Neural Network with Keras (Part 2) lab5_solutions.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Leonel-Jeffrey/COMP-1827/blob/main/Convolutional_Neural_Network_with_Keras_(Part_2)_lab5_solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y97A12EGJM5n"
      },
      "source": [
        "# Convolutional Neural Network with Keras (Part 2) \n",
        "\n",
        "**What is Keras?** Keras is a wrapper that allows you to implement Deep Neural Networks without getting into intrinsic details of the Network. It can use *Tensorflow* or *Theano* as backend. \n",
        "\n",
        "\n",
        "In this lab you will build the *VGG16 network* and explore *tranfer learning* for image classification (classify whether an image contains an airplane or automobile or bird or cat or deer or dog or frog or horse or ship or truck)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GmbVXIzJM5p"
      },
      "source": [
        "# Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-10T16:54:31.638667Z",
          "start_time": "2020-06-10T16:54:29.084221Z"
        },
        "id": "sZ_qSs-rJM5q"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import datasets, layers, models, optimizers\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSwAI53VJM5v"
      },
      "source": [
        "# Importing Dataset\n",
        "\n",
        "Here we are loading the cifar10 Dataset which is preloaded in tensorflow. <br>\n",
        "\n",
        "Calling the `load_data` function on this object returns splitted train and test data in form of (features, target)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-10T16:54:31.82622Z",
          "start_time": "2020-06-10T16:54:31.640652Z"
        },
        "id": "sN__Hw1sJM5v"
      },
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NY2BQittJM5z"
      },
      "source": [
        "# Overview of Dataset\n",
        "\n",
        "The CIFAR10 dataset contains 60,000 (32 x 32 pixel) color images in 10 classes, with 6,000 images in each class. The dataset is divided into 50,000 training images and 10,000 testing images. The classes are mutually exclusive and there is no overlap between them.<br>\n",
        ">The shape (50000, 32, 32, 3) represents **50000** images each of dimension **32 x 32 x 3**.<br>\n",
        "The shape **(50000, )** represents (50000, 1) shape i.e. 50000 labels, each for one image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-10T16:54:31.832844Z",
          "start_time": "2020-06-10T16:54:31.828686Z"
        },
        "id": "oF4wHayFJM50"
      },
      "source": [
        "print(f'Shape of the training data: {train_images.shape}')\n",
        "print(f'Shape of the training target: {train_labels.shape}')\n",
        "print(f'Shape of the test data: {test_images.shape}')\n",
        "print(f'Shape of the test target: {test_labels.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-10T16:54:31.839396Z",
          "start_time": "2020-06-10T16:54:31.835735Z"
        },
        "id": "qfKCqRKWJM53"
      },
      "source": [
        "print(train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-10T16:54:32.048741Z",
          "start_time": "2020-06-10T16:54:31.84145Z"
        },
        "id": "k8rWdBCGJM56"
      },
      "source": [
        "# To verify that the dataset looks correct, let's plot the first 16 images from the training set and display the class name below each image.\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "plt.figure(figsize=(32,32))\n",
        "for i in range(16):\n",
        "    plt.subplot(4,4,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
        "    # The CIFAR labels happen to be arrays, \n",
        "    # which is why you need the extra index\n",
        "    plt.xlabel(class_names[train_labels[i][0]])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m83ZM21PJM58"
      },
      "source": [
        "# Preprocessing\n",
        "\n",
        "Normalizing i.e. scaling the pixels to 0-1 from 0-255."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-10T16:54:32.275587Z",
          "start_time": "2020-06-10T16:54:32.05088Z"
        },
        "id": "i34Q3nNYJM59"
      },
      "source": [
        "# Normalizing\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9nZEGrvJM6A"
      },
      "source": [
        "# Modelling\n",
        "\n",
        "There are two types of models in Tensorflow:\n",
        " - **Sequential**\n",
        " - **Graphical**\n",
        "\n",
        "## Models\n",
        "`tf.keras.model.Sequential()` \n",
        "lets you create a linear stack of layers providing a Sequential netural network.<br>\n",
        "`tf.model()`\n",
        "allows you to create arbitarary graph of layers as long as there is no cycle.\n",
        "\n",
        "## Convolution Layer\n",
        "`tf.keras.layers.Conv2D()` Convolution layer takes the following argument\n",
        "> * **filters** Number of different types of convolutions used. Initially they are set to some predefined convolution and slowly trained to find better features in the image.\n",
        "* **kernel_size** An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions.\n",
        "* **strides** An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions. Default value is (1,1)\n",
        "* **padding** One of \"valid\" or \"same\" (case-insensitive): <br>\n",
        " VALID Padding: it means no padding and it assumes that all the dimensions are valid so that the input image gets fully covered by a filter and the stride specified by you. <br>\n",
        " SAME Padding: it applies padding to the input image so that the input image gets fully covered by the filter and specified stride. It is called SAME because, for stride 1 , the output will be the same as the input. \n",
        "* **activation** activation function. Default value is: None\n",
        "* **use_bias** Boolean, whether the layer uses a bias vector. Default value is: True\n",
        "* **input_shape** Size of each input to the convolution.\n",
        "\n",
        "## Pooling\n",
        "`tf.keras.layers.MaxPooling2D()` Max Pooling layer to reduce the size of the input. This layer takes the following arguments:\n",
        "> * **pool_size** Dimension of pooling kernel. Default value is (2, 2)\n",
        "* **strides** Integer, tuple of 2 integers, or None. Strides values. Specifies how far the pooling window moves for each pooling step. Default value is: None\n",
        "* **padding** One of \"valid\" or \"same\" (case-insensitive). \"valid\" adds no zero padding. \"same\" adds padding such that if the stride is 1, the output shape is the same as input shape. Default value is \"valid\".<br>\n",
        "`tf.keras.layers.AveragePooling2D()` Average Pooling layer to reduce the size of the input.\n",
        "\n",
        "\n",
        "## Flatten Layer\n",
        "`tf.keras.layers.Flatten()` flattens the input.<br>\n",
        "For input of `(batch_size, height, width, depth)` the output converts to `(batch_size, height * width * depth)`\n",
        "\n",
        "## Dense Layer\n",
        "`tf.keras.layers.Dense()` Normal dense layer (= fully connected layer): each node/neuron in this layer is connected to each node in the input layer. <br>\n",
        ">The two arguments passes below in dense layer are *units* and *activation* (activation function).<br>\n",
        "* **units** corresponds to the number of nodes in the layer<br>\n",
        "* **activation** is an element-wise activation function.\n",
        "    * **relu**: This activation function converts every negative value to 0 and positive remains the same\n",
        "    * **softmax**: This function takes as input a vector of K real numbers, and normalizes it into a probability distribution consisting of K probabilities proportional to the exponentials of the input numbers.\n",
        "\n",
        "\n",
        "## Compiling model\n",
        "`model.compile()` Sets up the optimiser, loss and metrics configuration.\n",
        "> * **optimizer**: updates the parameter of the Neural Network.\n",
        "* **loss**: Measures the error in our model.\n",
        "* **metrics**: Used to judge the model. The difference between metrics and loss is that metrics in not used to evaluate the model while training, whereas loss evaluates the model error while training and helps optimizer reduce the error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPzmEthPJM6B"
      },
      "source": [
        "# Creating by yourselves the VGG16\n",
        "\n",
        "As input, a CNN takes tensors of shape (image_height, image_width, color_channels), ignoring the batch size; color_channels refer to (R,G,B). \n",
        "In this example, we will build the VGG16 network and configure it to process inputs of shape (32, 32, 3). We can do this by passing the argument input_shape to our first layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-10T17:07:37.288653Z",
          "start_time": "2020-06-10T17:07:37.229297Z"
        },
        "id": "2Z43BeGFJM6B"
      },
      "source": [
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\", input_shape=(32,32,3)))\n",
        "model.add(layers.Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "model.add(layers.MaxPool2D(pool_size=(2,2),strides=(2,2)))  ## END BLOCK 1\n",
        "model.add(layers.Conv2D(filters=128,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "model.add(layers.Conv2D(filters=128,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "model.add(layers.MaxPool2D(pool_size=(2,2),strides=(2,2)))   ## END BLOCK 2\n",
        "model.add(layers.Conv2D(filters=256,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "model.add(layers.Conv2D(filters=256,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "model.add(layers.Conv2D(filters=256,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "model.add(layers.MaxPool2D(pool_size=(2,2),strides=(2,2)))    ## END BLOCK 3 \n",
        "model.add(layers.Conv2D(filters=512,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "model.add(layers.Conv2D(filters=512,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "model.add(layers.Conv2D(filters=512,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "model.add(layers.MaxPool2D(pool_size=(2,2),strides=(2,2)))    ## END BLOCK 4 \n",
        "model.add(layers.Conv2D(filters=512,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "model.add(layers.Conv2D(filters=512,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "model.add(layers.Conv2D(filters=512,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "model.add(layers.MaxPool2D(pool_size=(2,2),strides=(2,2)))    ## END BLOCK 5 \n",
        "model.add(layers.Flatten())    ## converting to vector \n",
        "model.add(layers.Dense(4096, activation=\"relu\"))    ## 1st FC layer \n",
        "model.add(layers.Dense(4096, activation=\"relu\"))    ## 2st FC layer \n",
        "model.add(layers.Dense(10, activation=\"softmax\"))    ## output layer \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model.compile(optimizers.Adam(learning_rate=0.0001),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPnvL5iV4rvK"
      },
      "source": [
        "# Using the built-in Keras code and weights of VGG16\n",
        "\n",
        "\n",
        "Keras provides access to a number of top-performing pre-trained models that were developed for image recognition tasks.\n",
        "\n",
        "They are available via the Applications API, and include functions to load a model with or without the pre-trained weights, and prepare data in a way that a given model may expect (e.g. scaling of size and pixel values).\n",
        "\n",
        "The first time a pre-trained model is loaded, Keras will download the required model weights, which may take some time given the speed of your internet connection. \n",
        "\n",
        "When loading a given model, the “include_top” argument can be set to False, in which case the model's fully-connected layers and the output layer, will not be loaded, allowing new layers to be added and trained. A model without a top will output activations from the last convolutional or pooling layer directly.\n",
        "Additionally, when the “include_top” argument is False, the “input_tensor” argument must be specified, allowing the expected fixed-sized input of the model to be changed. \n",
        "\n",
        "Alternately, we may wish to use the VGG16 model layers, but train the new layers of the model without updating the weights of the VGG16 layers (aka freeze these weights). This will allow the new added layers to learn to interpret the learned features of the VGG16.\n",
        "This can be achieved by setting the “trainable” property on each of the layers in the loaded VGG model to False prior to training. \n",
        "You can pick and choose which layers are trainable.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qY2zBnsKu0bl"
      },
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.models import Model\n",
        "\n",
        "# load model without classifier layers\n",
        "model = VGG16(include_top=False, weights=\"imagenet\", input_shape=(32, 32, 3)) \n",
        "    \n",
        "### If we want to freeze these pretrained weights:\n",
        "\n",
        "#for layer in model.layers:\n",
        "#\tlayer.trainable = False     # mark loaded layers as not trainable\n",
        "\n",
        "# add new classifier layers\n",
        "flat1 = layers.Flatten()(model.layers[-1].output)\n",
        "fc1 = layers.Dense(4096, activation='relu')(flat1)\n",
        "fc2 = layers.Dense(4096, activation='relu')(fc1)\n",
        "output = layers.Dense(10, activation='softmax')(fc2)\n",
        "\n",
        "\n",
        "# define new model\n",
        "model = Model(inputs=model.inputs, outputs=output)\n",
        "\n",
        "model.compile(optimizers.Adam(learning_rate=0.0001),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4Y6hbfXJM6E"
      },
      "source": [
        "## Model details\n",
        "\n",
        "Let's look at details of the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-10T17:24:41.162476Z",
          "start_time": "2020-06-10T17:24:41.157906Z"
        },
        "id": "WccMxDoOJM6E"
      },
      "source": [
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjKoESaaJM6H"
      },
      "source": [
        "## Training\n",
        "\n",
        "```model.fit``` trains the model.\n",
        "> * **train_images**: Training data/features\n",
        "* **train_labels**: Target\n",
        "* **epochs**: Number of times the entire dataset is fed in the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-10T16:59:03.569655Z",
          "start_time": "2020-06-10T16:54:32.3542Z"
        },
        "id": "1zXaQqKqJM6H"
      },
      "source": [
        "# Training\n",
        "history = model.fit(train_images, train_labels, epochs=10, batch_size=128,\n",
        "                    validation_data=(test_images, test_labels))\n",
        "\n",
        "# Validation\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0.5, 1])\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
        "\n",
        "print(test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTbZ7Y8uJM6J"
      },
      "source": [
        "## Visualize prediction\n",
        "\n",
        "Now let's visualize the prediction using the model you just trained. \n",
        "First we get the predictions with the model from the test data.\n",
        "Then we print out 15 images from the test data set, and set the titles with the prediction (and the ground truth label).\n",
        "If the prediction matches the true label, the title will be green; otherwise it's displayed in red."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xavvSORHJM6K"
      },
      "source": [
        "import pdb\n",
        "y_hat = model.predict(test_images)\n",
        "\n",
        "# Plot a random sample of 15 test images, their predicted labels and ground truth\n",
        "figure = plt.figure(figsize=(20, 20))\n",
        "for i, index in enumerate(np.random.choice(test_images.shape[0], size=15, replace=False)):\n",
        "    ax = figure.add_subplot(3, 5, i + 1, xticks=[], yticks=[])\n",
        "    # Display each image\n",
        "    ax.imshow(np.squeeze(test_images[index]))\n",
        "    predict_index = np.argmax(y_hat[index])\n",
        "    true_index = test_labels[index][0]\n",
        "    # Set the title for each image\n",
        "    ax.set_title(\"{} ({})\".format(class_names[predict_index], \n",
        "                                  class_names[true_index]),\n",
        "                                  color=(\"green\" if predict_index == true_index else \"red\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8BZCkYBJM6P"
      },
      "source": [
        "# Visualising feature maps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-06-10T16:59:05.523866Z",
          "start_time": "2020-06-10T16:59:03.572384Z"
        },
        "id": "aKprwqLxJM6P"
      },
      "source": [
        "from numpy import expand_dims\n",
        "from keras.models import Model\n",
        "\n",
        "# redefine model to output right after the first conv layer\n",
        "model1 = Model(inputs=model.inputs, outputs=model.layers[1].output)   ### just put any index between 1 and 18 (to visualise the conv and pooling layers)\n",
        "model1.summary()\n",
        "# load the image with the required shape\n",
        "img = train_images[0]   ### if you want change the index here to load another image from the training set\n",
        "# expand dimensions so that it represents a single 'sample'\n",
        "img = expand_dims(img, axis=0)\n",
        "# get feature map for first conv layer\n",
        "feature_maps = model1.predict(img)\n",
        "# plot all 64 maps in 8x8 squares   ### square1 times square 2 should always be the number of output feature maps of the layer \n",
        "### (in other words should be the last number of the 'output shape' column of the model summary)\n",
        "square1 = 8\n",
        "square2 = 8\n",
        "ix = 1\n",
        "\n",
        "# Display the input image\n",
        "plt.imshow(img[0])\n",
        "\n",
        "\n",
        "figure = plt.figure(figsize=(20, 20))\n",
        "for _ in range(square1):\n",
        "\tfor _ in range(square2):\n",
        "\t\tax = figure.add_subplot(square1, square2, ix, xticks=[], yticks=[])\n",
        "    # Display each image\n",
        "\t\t# plot filter channel in grayscale\n",
        "\t\tax.imshow(feature_maps[0, :, :, ix-1], cmap='gray')\n",
        "\t\tix += 1\n",
        "# show the figure\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}